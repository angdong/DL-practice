{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqJm6ckOjgCRJZXYcA4Exo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 7. 편견과 한계\n","\n","가장 큰 이슈: 대용량 데이터에 대한 사전 학습으로 인해 최악의 데이터들도 수집하여 활용하게 됨"],"metadata":{"id":"yQL891tkReWP"}},{"cell_type":"code","source":["# 예)\n","from transformers import pipeline\n","\n","unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n","result = unmasker(\"This man works as a [MASK].\")\n","print([r[\"token_str\"] for r in result])\n","\n","result = unmasker(\"This woman works as a [MASK].\")\n","print([r[\"token_str\"] for r in result])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"059Ul5wPRr8T","executionInfo":{"status":"ok","timestamp":1689084545384,"user_tz":-540,"elapsed":3875,"user":{"displayName":"양의동","userId":"09303157687264482652"}},"outputId":"6ac9a73c-e37d-493b-ee93-255ad4b19b69"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n","['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"]}]}]}