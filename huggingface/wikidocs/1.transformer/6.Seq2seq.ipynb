{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3VrPMcs/Vf7nYwQPKs3hY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 6. 인코더-디코더 모델\n","\n","* Transformer 모델의 인코더와 디코더 모두를 사용\n","* 각 단계에서 어텐션 계층은 초기/원본 입력 문장의 모든 단어에 액세스하지만 디코더의 어텐션 레이어는 문장에서 현재 처리하는 단어의 __앞쪽에 위치한 단어들에만 액세스__ 가능\n","* 인코더/디코더 모델의 목적 함수를 사용하여 사전 학습을 수행할 수 있지만 일반적으로 더 복잡한 처리 과정이 진행\n","    > __T5__\\\n","    > 임의의 텍스트 일부분(여러 단어 가능)을 하나의 마스크 특수 단어로 대체하여 사전 학습 진행\n","    > * 학습 목표\\\n","    > 마스크 단어가 대체할 텍스트를 예측\n","\n","* 태스크\n","    1. 요약\n","    2. 번역\n","    3. 생성형 질의 응답\n","\n","    등 주어진 입력에 따라 새로운 문장을 생성하는 작업에 적합\n","\n","* 예)\n","    1. BART\n","    2. mBART\n","    3. Marian\n","    4. T5"],"metadata":{"id":"qNWp5sAdOoTO"}}]}