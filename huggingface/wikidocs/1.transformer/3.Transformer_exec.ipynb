{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMjwqsDNaJmNGJCeHTppcLw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 3. Transformers는 어떻게 동작할까?\n","\n","### 역사\n","1. 18.6: GPT\n","2. 18.10: BERT\n","3. 19.2: GPT-2\n","4. 19.10: DistillBERT\n","5. 19.10: BART, T5\n","6. 20.5: GPT3\n","\n","크게 3가지 범주\n","\n","1. GPT-like(auto-regressive Transformer)\n","2. BERT-like(auto-encoding Transformer)\n","3. BART/T5-like(seq2seq Transformer)\n","\n","### Language Model\n","\n","앞선 Transformer모델들은 언어 모델(자가 지도)로 학습되었다. -> 사람이 데이터에 레이블을 지정할 필요가 없다.\n","\n","언어에 대한 통계적인 이해는 가능하지만 실제 태스크에는 크게 유용하지 않다.\n","\n","* 따라서 전이학습 프로세스를 거치며 supervised 방식으로 미세 조정된다."],"metadata":{"id":"EBUK-q9Paikh"}},{"cell_type":"markdown","source":["### 전이 학습\n","\n","![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg)\n","\n","미세 조정: 모델이 사전 학습된 후에 수행되는 학습, 특정 태스크를 위해 학습\n","\n","* 미세 조정 과정에서 사전 학습때 얻은 지식을 활용\n","* 이미 많은 데이터에 대해 사전 학습되었기 때문에 적은 데이터셋에서도 좋은 결과를 얻을 수 있음\n","\n","![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg)"],"metadata":{"id":"Cl9cVZ6nI2_K"}},{"cell_type":"markdown","source":["### 일반적인 아키텍처\n","\n","모델의 주요 블록\n","\n","* 인코더: 입력에 대한 representation 및 feature 도출, 입력으로부터 이해를 얻기\\\n"," -> 최종 목적 태스크를 위해서 __입력에 대한 표현 형태 최적화__\n","\n","* 디코더: 인코더가 구성한 representation 및 feature를 다른 입력과 함께 사용해서 대상 시퀀스를 생성\\\n"," -> __출력 생성에 최적화__\n","\n","1. 인코더 전용 모델: 문장 분류 및 개체명 인식 등 입력에 대한 분석 및 이해 태스크\n","2. 디코더 전용 모델: 텍스트 생성 등과 같은 생성 태스크\n","3. 인코더-디코더 모델: 번역, 요약과 같은 입력이 수반되는 생성 태스크"],"metadata":{"id":"tZo6U9GgKySK"}},{"cell_type":"markdown","source":["### 오리지널 아키텍처\n","\n","* Transformer: 원래 번역용으로 설계\n","    \n","    인코더: 특정 언어로 표기된 문장 수신\\\n","    디코더: 원하는 대상 언어로 표기된 동일한 의미의 문장 수신\n","\n","학습 중 속도를 높이기 위해 디코더는 전체 대상 문장을 입력받지만, 미래 단어를 사용하는 것은 허용되지 않는다.\n","\n","_어텐션 마스크_ : 인코더/디코더에서 모델이 특정 단어에 주의 집중하는 것을 방지하는 데에 사용"],"metadata":{"id":"q-4XzjZaLmCh"}},{"cell_type":"markdown","source":["### 아키텍처 vs 체크포인트\n","\n","* 아키텍처: 모델의 skeleton, layer와 operation 등을 정의\n","* 체크포인트: 아키텍처에 로드될 가중치 값들\n","* 모델: 포괄적인 용어\n"],"metadata":{"id":"VyzZC0VmMp6p"}}]}