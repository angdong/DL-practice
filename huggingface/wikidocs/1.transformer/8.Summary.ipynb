{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO81dzotJGBBC0XYAPWZXdg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 8. 1장 요약\n","\n","* Transformers 의 high-leve `pipeline` 함수 사용하여 다양한 NLP 작업 수행\n","* 아키텍처 예시\n","\n","|모델|예시|태스크|\n","|---|---|----|\n","|인코더|ALBERT, BERT, DistillBERT, ELECTRA, RoBERTa|문장 분류, NER, 추출형 질의 응답\n","|디코더|CTRL, GPT, GPT-2, Transformer-XL|텍스트 생성|\n","|인코더-디코더|BART, T5, Marian, mBART|요약, 번역, 생성형 질의응답|"],"metadata":{"id":"DHbzNlxpSReM"}}]}