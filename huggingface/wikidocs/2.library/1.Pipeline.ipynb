{"cells":[{"cell_type":"markdown","metadata":{"id":"o9jgQoCGXgtX"},"source":["### Pipeline 내부 실행"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n","pip install xformers.\n"]},{"data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n"," {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")\n","\n","classifier(\n","    [\n","        \"I've been waiting for a HuggingFace course my whole life.\",\n","        \"I hate this so much!\"\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["내부 수행 과정\n","\n","![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"]},{"cell_type":"markdown","metadata":{},"source":["### 토크나이저를 이용한 전처리\n","\n","파이프라인의 첫번째 단계\\\n","텍스트 입력을 모델이 이해할 수 있는 숫자로 변환하기\n","\n","토크나이저\n","* 입력을 토큰으로 분할\n","* 각 토큰을 정수로 매핑\n","* 모델에 유용할 수 있는 부가적인 입력 추가\n","\n","`AutoTokenizer` `from_pretrained()` 를 사용하여 모델이 사전 학습될 때와 동일한 방식으로 수행"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"markdown","metadata":{},"source":["tokenizer 생성하면 토크나이저에 문장을 입력해서 모델에 전달할 수 있는 파이썬 딕셔너리 정보를 구할 수 있다.\n","\n","이후에 input IDs 리스트를 텐서로 변환해주면 된다"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n","          2607,  2026,  2878,  2166,  1012,   102],\n","        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"]}],"source":["# 텐서의 유형 지정하기\n","raw_inputs = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"I hate this so much!\",\n","]\n","inputs = tokenizer(\n","    raw_inputs,\n","    padding=True,\n","    truncation=True,\n","    return_tensors=\"pt\"\n",")\n","print(inputs)"]},{"cell_type":"markdown","metadata":{},"source":["출력은 두개의 키를 가지는 파이썬 딕셔너리이다.\n","1. `input_ids` 토큰의 고유 식별자로 구성된 두 행의 정수\n","2. `attention_mask`"]},{"cell_type":"markdown","metadata":{},"source":["### 모델"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import AutoModel\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModel.from_pretrained(checkpoint)"]},{"cell_type":"markdown","metadata":{},"source":["### 고차원 벡터\n","\n","Transformer 모듈이 가지는 일반적인 차원\n","1. 배치 크기\n","2. 시퀀스 길이\n","3. 은닉 크기\n","\n","Trnasformer 모델의 출력은 `namedtuple` 이나 딕셔너리처럼 동작\n","\n","접근 방법\n","1. `outputs[\"last_hidden_state\"]`\n","2. `outputs[0]`"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 16, 768])\n"]}],"source":["outputs = model(**inputs)\n","print(outputs.last_hidden_state.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### 모델 헤드\n","\n","![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)\n","\n","Transformers는 다양한 아키텍처가 있고, 각 아키텍처는 특화된 작업을 처리하도록 설계되어 있다\n","\n","1. Model\n","2. ForCausalLM\n","3. ForMaskedLM\n","...\n","\n","예시에서는 문장의 긍정/부정을 분류하기 위해 시퀀스 분류 헤드가 포함되어 있는 모델이 필요하다.\\\n","따라서 `AutoModel` 클래스 대신 `AutoModelForSequenceClassification` 을 사용한다."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","outputs = model(**inputs)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 2])\n"]}],"source":["# shape 보기\n","# 두 개의 문장, 두 개의 레이블 -> (2,2) shape\n","print(outputs.logits.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### 출력 후처리\n","\n","정규화되지 않은 원시 점수인 __logits__ 을 확률로 변환하는 과정이 필요하다\n","\n","__SoftMax__ 계층 통과시키기"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[4.0195e-02, 9.5980e-01],\n","        [9.9946e-01, 5.4419e-04]], grad_fn=<SoftmaxBackward0>)\n"]}],"source":["import torch\n","\n","predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","print(predictions)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["{0: 'NEGATIVE', 1: 'POSITIVE'}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# 해당하는 레이블 가져오기\n","model.config.id2label"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOqW8kuNftARk/stuoHUFcL","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
