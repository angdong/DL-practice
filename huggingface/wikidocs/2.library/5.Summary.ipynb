{"cells":[{"cell_type":"markdown","metadata":{"id":"VZl4B0k-X5JA"},"source":["## 요약\n","\n","살펴본 것\n","\n","1. 토큰화\n","2. 입력 식별자로의 변환(input IDs)\n","3. 패딩\n","4. 절단\n","5. 어텐션 마스크\n","\n","$\\rightarrow$ Transformers API를 사용해서 high-level 함수로 처리해보자"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)"]},{"cell_type":"markdown","metadata":{},"source":["`model_inputs` 에는 input IDs와 attention mask가 포함된다"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# 단일 시퀀스 토큰화\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# 한 번에 여러 시퀀스 토큰화\n","sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","model_inputs = tokenizer(sequences)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# 다양한 모드에 따라 패딩 처리\n","\n","# 시퀀스를 리스트 내의 최대 시퀀스 길이까지 패딩\n","model_inputs = tokenizer(sequences, padding=\"longest\")\n","\n","# 시퀀스를 최대 모델 길이까지 패딩\n","model_inputs = tokenizer(sequences, padding=\"max_length\")\n","\n","# 지정된 최대 길이까지 시퀀스를 패딩\n","model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# truncation\n","\n","sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","# 모델 최대 길이보다 긴 시퀀스 자르기\n","model_inputs = tokenizer(sequences, truncation=True)\n","\n","# 지정된 최대 길이보다 긴 시퀀스 자르기\n","model_inputs = tokenizer(sequences, max_length=8, truncation=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n","[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"]}],"source":["# 특수 토큰들\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)\n","print(model_inputs[\"input_ids\"])\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(ids)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n","i've been waiting for a huggingface course my whole life.\n"]}],"source":["# id sequence를 디코딩\n","\n","print(tokenizer.decode(model_inputs[\"input_ids\"]))\n","print(tokenizer.decode(ids))"]},{"cell_type":"markdown","metadata":{},"source":["토크나이저는 특수 단어들을 추가한다\\\n","모델이 해당 특수 토큰들로 학습되었기 때문에 추론에서 동일한 결과를 얻으려면 이를 추가해야 한다"]},{"cell_type":"markdown","metadata":{},"source":["### 요약"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n","        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","tokens = tokenizer(\n","    sequences,\n","    padding=True,\n","    truncation=True,\n","    return_tensors=\"pt\"\n",")\n","output = model(**tokens)\n","print(output)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOqW8kuNftARk/stuoHUFcL","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
