{"cells":[{"cell_type":"markdown","metadata":{"id":"p8cb84CcXtZa"},"source":["## 모델\n","\n","체크포인트를 바탕으로 모델을 인스턴스화할 때 편리한 `AutoModel` 클래스 사용\n","\n","* `AutoModel` 클래스\n","\n","다양한 모델에 대한 단순한 wrapper\\\n","체크포인트에 적합한 모델 아키텍처를 자동으로 추측한 뒤 모델을 인스턴스화함"]},{"cell_type":"markdown","metadata":{},"source":["### 트랜스포머 모델 생성\n","\n","BERT 모델을 초기화하기 위해 가장 먼저 configuration 객체를 로드해야 함"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from transformers import BertConfig, BertModel\n","\n","# config(설정)을 만들기\n","config = BertConfig()\n","\n","# 해당 config에서 모델을 생성\n","model = BertModel(config)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.30.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}],"source":["# configuration 객체에는 모델을 빌드하는데 필요한 속성이 포함됨\n","print(config)"]},{"cell_type":"markdown","metadata":{},"source":["### 다른 로딩 메소드들\n","\n","기본 설정에서 모델을 생성하면 해당 모델을 임의의 값으로 초기화\n","\n","```python\n","from transformers import BertConfig, BertModel\n","\n","config = BertConfig()\n","model = BertModel(config)\n","\n","# 모델은 무작위로 초기화\n","```\n","\n","사전 학습된 Transformer 모델을 로드하여 사용"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 103kB/s]\n","Downloading model.safetensors: 100%|██████████| 436M/436M [00:43<00:00, 10.0MB/s] \n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import BertModel\n","\n","model = BertModel.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"markdown","metadata":{},"source":["`BertModel` 을 동일한 기능을 수행하는 `AutoModel` 클래스로 대체 가능\n","\n","위의 코드는 `BertConfig` 를 사용하지 않고 `bert-base-cased` 식별자를 통해 모델을 로드함\\\n","해당 모델은 체크포인트의 모든 가중치로 초기화되고, 새로운 작업에 대해 미세 조정할 수 있다\n","\n","자동으로 가중치가 다운로드되고 캐시되어 캐시 폴더에 저장\n"]},{"cell_type":"markdown","metadata":{},"source":["### 저장 메소드\n","\n","config.json: 모델 아키텍처를 구축하는 데 필요한 다양한 속성들\\\n","pytorch_model.bin: `state dictionay` 라고도 불림, 모델의 모든 가중치가 저장됨\n","\n","설정 객체는 모델의 아키텍처를 파악하는데 필요\\\n","모델 가중치는 모델의 매개변수"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# from_pretrained()와 유사한 save_pretrained() 메서드를 사용\n","model.save_pretrained(\"saving_folder\")"]},{"cell_type":"markdown","metadata":{},"source":["### 트랜스포머 모델을 활용한 추론\n","\n","모델을 사용하여 몇 가지 예측 수행\n","\n","예) 두 개의 시퀀스\n","\n","```python\n","sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n","```\n","\n","토크나이저는 이를 input IDs 라고 하는 어휘 인덱스로 변환한다\\\n","출력은 다음과 같다\n","\n","```python\n","encoded_sequences = [\n","    [101, 7592, 999, 102],\n","    [101, 4658, 1012, 102],\n","    [101, 3835, 999, 102],\n","]\n","```"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["encoded_sequences = [\n","    [101, 7592, 999, 102],\n","    [101, 4658, 1012, 102],\n","    [101, 3835, 999, 102],\n","]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# 모델의 입력으로 텐서 활용\n","# 입력을 매개변수로 지정하여 모델을 호출하기\n","\n","import torch\n","\n","model_inputs = torch.tensor(encoded_sequences)\n","output = model(model_inputs)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.4496e-01,  4.8276e-01,  2.7797e-01,  ..., -5.4032e-02,\n","           3.9393e-01, -9.4770e-02],\n","         [ 2.4943e-01, -4.4093e-01,  8.1772e-01,  ..., -3.1917e-01,\n","           2.2992e-01, -4.1172e-02],\n","         [ 1.3668e-01,  2.2518e-01,  1.4502e-01,  ..., -4.6915e-02,\n","           2.8224e-01,  7.5566e-02],\n","         [ 1.1789e+00,  1.6739e-01, -1.8187e-01,  ...,  2.4671e-01,\n","           1.0441e+00, -6.1966e-03]],\n","\n","        [[ 3.6436e-01,  3.2464e-02,  2.0258e-01,  ...,  6.0111e-02,\n","           3.2451e-01, -2.0995e-02],\n","         [ 7.1866e-01, -4.8725e-01,  5.1740e-01,  ..., -4.4012e-01,\n","           1.4553e-01, -3.7545e-02],\n","         [ 3.3223e-01, -2.3271e-01,  9.4876e-02,  ..., -2.5268e-01,\n","           3.2172e-01,  8.1120e-04],\n","         [ 1.2523e+00,  3.5754e-01, -5.1320e-02,  ..., -3.7840e-01,\n","           1.0526e+00, -5.6255e-01]],\n","\n","        [[ 2.4042e-01,  1.4718e-01,  1.2110e-01,  ...,  7.6062e-02,\n","           3.3564e-01,  2.8262e-01],\n","         [ 6.5701e-01, -3.2787e-01,  2.4968e-01,  ..., -2.5920e-01,\n","           2.0175e-01,  3.3275e-01],\n","         [ 2.0160e-01,  1.5783e-01,  9.8972e-03,  ..., -3.8850e-01,\n","           4.1307e-01,  3.9732e-01],\n","         [ 1.0175e+00,  6.4387e-01, -7.8147e-01,  ..., -4.2109e-01,\n","           1.0925e+00, -4.8456e-02]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6856,  0.5262,  1.0000,  ...,  1.0000, -0.6112,  0.9971],\n","        [-0.6055,  0.4997,  0.9998,  ...,  0.9999, -0.6753,  0.9769],\n","        [-0.7702,  0.5447,  0.9999,  ...,  1.0000, -0.4655,  0.9894]],\n","       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["output"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOqW8kuNftARk/stuoHUFcL","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
