{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNvUqDUdU27xUlBiR+1aBGc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["[모델 매개변수 최적화하기](https://tutorials.pytorch.kr/beginner/basics/optimization_tutorial.html)"],"metadata":{"id":"27_uaPv52WCg"}},{"cell_type":"markdown","source":["## 모델 매개변수 최적화하기"],"metadata":{"id":"vIrOZp_cAKor"}},{"cell_type":"code","source":["\"\"\"\n","데이터에 매개변수를 최적화하여 모델 학습하고, 검증하고, 테스트 진행\n","각 반복 단계:\n","    모델이 출력을 추출\n","    모델이 추측과 정답 사이의 오류를 계산\n","    매개변수에 대한 오류의 도함수 수집\n","    경사하강법을 사용해 파라미터들 최적화\n","\"\"\"\n","\n","# 기본 코드\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor(),\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor(),\n",")\n","\n","train_dataloader = DataLoader(training_data, batch_size=64)\n","test_dataloader = DataLoader(test_data, batch_size=64)\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NeuralNetwork()"],"metadata":{"id":"VPawxQ8IAUqa","executionInfo":{"status":"ok","timestamp":1687792147056,"user_tz":-540,"elapsed":294,"user":{"displayName":"양의동","userId":"09303157687264482652"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## 하이퍼파라미터"],"metadata":{"id":"_MQU4NC8AaIT"}},{"cell_type":"code","source":["\"\"\"\n","모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수\n","서로 다른 하이퍼파라미터 값은 모델 학습과 수렴율에 영향 미침\n","\n","학습 시에 정의하는 하이퍼파라미터\n","    1. 에폭 수: 데이터셋 반복 정도\n","    2. 배치 크기: 매개변수 갱신 전 신경망을 통해 전파된 데이터 샘플 수\n","    3. 학습률: 각 배치/에폭에서 매개변수 조절 비율\n","\"\"\"\n","\n","learning_rate = 1e-3\n","batch_size = 64\n","epochs = 5"],"metadata":{"id":"ooTAEfY2BTv7","executionInfo":{"status":"ok","timestamp":1687792147486,"user_tz":-540,"elapsed":3,"user":{"displayName":"양의동","userId":"09303157687264482652"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## 최적화 단계\n","\n","epoch: 최적화 단계의 각 반복\n","\n","* 학습 단계: 학습용 데이터셋 반복, 최적 매개변수로 수렴\n","* 검증/테스트 단계: 모델 성능 개선 확인"],"metadata":{"id":"JfuMVlI3BlMZ"}},{"cell_type":"markdown","source":["### 손실 함수\n","\n","획득한 결과와 실제 값 사이의 틀린 정도를 측정하여 학습 중 이 값을 최소화\n","\n","주어진 데이터 샘플을 입력으로 계산한 예측과 정답을 비교하여 손실을 계산\n","\n","일반적인 손실함수\n","\n","1. 회귀 문제의 `nn.MSELoss`\n","2. 분류 문제의 `nn.NLLLoss`\n","3. `nn.CrossEntropyLoss`: `nn.LogSoftmax` + `nn.NLLLoss`"],"metadata":{"id":"eKtsp0kZBmUo"}},{"cell_type":"code","source":["# 손실 함수 초기화\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"k44dukt3I1OK","executionInfo":{"status":"ok","timestamp":1687792147486,"user_tz":-540,"elapsed":3,"user":{"displayName":"양의동","userId":"09303157687264482652"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### 옵티마이저\n","\n","최적화 알고리즘: 모델의 오류를 줄이기 위해 모델 매개변수를 조절하는 과정이 수행되는 방식을 정의\n","\n","모든 최적화 절차는 `optimizer` 객체에 캡슐화된다"],"metadata":{"id":"vQy9z-wjI4zN"}},{"cell_type":"code","source":["# 학습하려는 모델의 매개변수와 학습률 하이퍼파라미터를 등록하여 옵티마이저 초기화\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"metadata":{"id":"BILFHSTXJD1e","executionInfo":{"status":"ok","timestamp":1687792147486,"user_tz":-540,"elapsed":2,"user":{"displayName":"양의동","userId":"09303157687264482652"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["학습 단계의 최적화\n","\n","1. `optimizer.zero_grad`: 모델 매개변수의 변화도 재설정, 중복 계산을 막기 위해 명시적으로 0으로 설정\n","2. `loss.backward()`: 예측 손실을 역전파\n","3. `optimizer.step()`: 역전파 단계에서 수집된 변화도로 매개변수 조정"],"metadata":{"id":"PnZCGdR_JJ8e"}},{"cell_type":"code","source":["# 전체 구현\n","\n","def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","\n","    for batch, (X, y) in enumerate(dataloader):\n","        # 예측 및 손실 계산\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # 역전파\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), (batch + 1) * len(X)\n","            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n","\n","def test_loop(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred=model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n"],"metadata":{"id":"3sXidx5TJW0e","executionInfo":{"status":"ok","timestamp":1687792835502,"user_tz":-540,"elapsed":262,"user":{"displayName":"양의동","userId":"09303157687264482652"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","epochs = 10\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\", \"-\"*20)\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    test_loop(test_dataloader, model, loss_fn)\n","print(\"Done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwYWCgUWQGb-","executionInfo":{"status":"ok","timestamp":1687792977234,"user_tz":-540,"elapsed":140491,"user":{"displayName":"양의동","userId":"09303157687264482652"}},"outputId":"cb1ac764-f92b-4ba1-f31b-2626d40b49d3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 --------------------\n","loss: 2.181453 [   64/60000]\n","loss: 2.176692 [ 6464/60000]\n","loss: 2.124108 [12864/60000]\n","loss: 2.145625 [19264/60000]\n","loss: 2.109811 [25664/60000]\n","loss: 2.039901 [32064/60000]\n","loss: 2.092655 [38464/60000]\n","loss: 2.004670 [44864/60000]\n","loss: 2.006806 [51264/60000]\n","loss: 1.959471 [57664/60000]\n","Test Error: \n"," Accuracy: 56.1%, Avg loss: 1.944161\n","\n","Epoch 2 --------------------\n","loss: 1.965254 [   64/60000]\n","loss: 1.944948 [ 6464/60000]\n","loss: 1.835641 [12864/60000]\n","loss: 1.884332 [19264/60000]\n","loss: 1.778961 [25664/60000]\n","loss: 1.713086 [32064/60000]\n","loss: 1.768803 [38464/60000]\n","loss: 1.643926 [44864/60000]\n","loss: 1.670038 [51264/60000]\n","loss: 1.576959 [57664/60000]\n","Test Error: \n"," Accuracy: 61.4%, Avg loss: 1.577529\n","\n","Epoch 3 --------------------\n","loss: 1.631402 [   64/60000]\n","loss: 1.597851 [ 6464/60000]\n","loss: 1.448930 [12864/60000]\n","loss: 1.526762 [19264/60000]\n","loss: 1.398388 [25664/60000]\n","loss: 1.383843 [32064/60000]\n","loss: 1.423208 [38464/60000]\n","loss: 1.321696 [44864/60000]\n","loss: 1.358727 [51264/60000]\n","loss: 1.260333 [57664/60000]\n","Test Error: \n"," Accuracy: 63.8%, Avg loss: 1.277595\n","\n","Epoch 4 --------------------\n","loss: 1.350844 [   64/60000]\n","loss: 1.329334 [ 6464/60000]\n","loss: 1.168162 [12864/60000]\n","loss: 1.273521 [19264/60000]\n","loss: 1.149150 [25664/60000]\n","loss: 1.167627 [32064/60000]\n","loss: 1.206435 [38464/60000]\n","loss: 1.123640 [44864/60000]\n","loss: 1.163944 [51264/60000]\n","loss: 1.078301 [57664/60000]\n","Test Error: \n"," Accuracy: 64.8%, Avg loss: 1.095049\n","\n","Epoch 5 --------------------\n","loss: 1.163980 [   64/60000]\n","loss: 1.163720 [ 6464/60000]\n","loss: 0.989516 [12864/60000]\n","loss: 1.120807 [19264/60000]\n","loss: 1.000496 [25664/60000]\n","loss: 1.026296 [32064/60000]\n","loss: 1.076529 [38464/60000]\n","loss: 1.001254 [44864/60000]\n","loss: 1.042587 [51264/60000]\n","loss: 0.968225 [57664/60000]\n","Test Error: \n"," Accuracy: 65.7%, Avg loss: 0.981549\n","\n","Epoch 6 --------------------\n","loss: 1.038407 [   64/60000]\n","loss: 1.059806 [ 6464/60000]\n","loss: 0.871399 [12864/60000]\n","loss: 1.022701 [19264/60000]\n","loss: 0.910758 [25664/60000]\n","loss: 0.930037 [32064/60000]\n","loss: 0.993967 [38464/60000]\n","loss: 0.923334 [44864/60000]\n","loss: 0.961899 [51264/60000]\n","loss: 0.896328 [57664/60000]\n","Test Error: \n"," Accuracy: 67.2%, Avg loss: 0.906565\n","\n","Epoch 7 --------------------\n","loss: 0.949016 [   64/60000]\n","loss: 0.990053 [ 6464/60000]\n","loss: 0.788789 [12864/60000]\n","loss: 0.955545 [19264/60000]\n","loss: 0.853024 [25664/60000]\n","loss: 0.861518 [32064/60000]\n","loss: 0.937842 [38464/60000]\n","loss: 0.872066 [44864/60000]\n","loss: 0.905108 [51264/60000]\n","loss: 0.846288 [57664/60000]\n","Test Error: \n"," Accuracy: 68.3%, Avg loss: 0.853871\n","\n","Epoch 8 --------------------\n","loss: 0.881982 [   64/60000]\n","loss: 0.939404 [ 6464/60000]\n","loss: 0.727946 [12864/60000]\n","loss: 0.906662 [19264/60000]\n","loss: 0.813005 [25664/60000]\n","loss: 0.810846 [32064/60000]\n","loss: 0.896839 [38464/60000]\n","loss: 0.836786 [44864/60000]\n","loss: 0.863131 [51264/60000]\n","loss: 0.809011 [57664/60000]\n","Test Error: \n"," Accuracy: 69.5%, Avg loss: 0.814535\n","\n","Epoch 9 --------------------\n","loss: 0.828944 [   64/60000]\n","loss: 0.899590 [ 6464/60000]\n","loss: 0.680664 [12864/60000]\n","loss: 0.869585 [19264/60000]\n","loss: 0.782785 [25664/60000]\n","loss: 0.772326 [32064/60000]\n","loss: 0.864630 [38464/60000]\n","loss: 0.811213 [44864/60000]\n","loss: 0.830529 [51264/60000]\n","loss: 0.779410 [57664/60000]\n","Test Error: \n"," Accuracy: 70.6%, Avg loss: 0.783628\n","\n","Epoch 10 --------------------\n","loss: 0.785424 [   64/60000]\n","loss: 0.866244 [ 6464/60000]\n","loss: 0.642513 [12864/60000]\n","loss: 0.840368 [19264/60000]\n","loss: 0.758521 [25664/60000]\n","loss: 0.742160 [32064/60000]\n","loss: 0.837737 [38464/60000]\n","loss: 0.791310 [44864/60000]\n","loss: 0.804091 [51264/60000]\n","loss: 0.754891 [57664/60000]\n","Test Error: \n"," Accuracy: 72.0%, Avg loss: 0.758101\n","\n","Done!\n"]}]}]}