{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"5rkL-AcmjHfc"},"source":["[TORCHTEXT 라이브러리로 텍스트 분류하기](https://tutorials.pytorch.kr/beginner/text_sentiment_ngrams_tutorial.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1688392211215,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"684v86FI4M8c","outputId":"990957b3-b502-4ed2-c726-6f98bce6bb8a"},"outputs":[],"source":["\"\"\"\n","텍스트 분류\n","\n","반복자(iterator)로 가공되지 않은 데이터에 접근\n","가공되지 않은 텍스트 문장들을 모델 학습에 사용할 수 있게 torch.Tensor로 변환하는 데이터 처리 파이프라인 생성\n","torch.utils.data.DataLoader를 사용하여 데이터를 섞고 반복하기\n","\"\"\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iiBJ0Z2U-_gb"},"source":["## 기초 데이터셋 반복자에 접근하기"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4982,"status":"ok","timestamp":1688392216621,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"Oh-y0lJV_Y2z","outputId":"22c31099-0609-4741-c0b9-98a85a633199"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.7.0)\n"]}],"source":["!pip install portalocker"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3201,"status":"ok","timestamp":1688392224766,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"PMcm31xR_FEf"},"outputs":[],"source":["\"\"\"\n","torchtext 라이브러리: 가공되지 않은 텍스트 문장들을 만드는 데이터셋 반복자 제공\n","    예) 레이블과 문장의 튜플 형태로 가공되지 않은 데이터 생성\n","\"\"\"\n","\n","import torch\n","from torchtext.datasets import AG_NEWS\n","train_iter = iter(AG_NEWS(split='train'))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3514,"status":"ok","timestamp":1688392228275,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"AVBUXqt1_TfB","outputId":"a89bdfce-7a14-48e8-be39-b090f2c24b61"},"outputs":[{"data":{"text/plain":["(3,\n"," \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["next(train_iter)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1688392244312,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"cQyrROSj_c9n","outputId":"5ba450ad-196e-4a5a-d55d-5b49e33c9f3d"},"outputs":[{"data":{"text/plain":["(3,\n"," 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["next(train_iter)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":444,"status":"ok","timestamp":1688392247745,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"pYy5Wa_5BRk1","outputId":"adf152d0-fd82-47a1-e473-d7c97156fd37"},"outputs":[{"data":{"text/plain":["(3,\n"," \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["next(train_iter)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SJH1XjCrBSSD"},"source":["## 데이터 처리 파이프라인 준비"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5402,"status":"ok","timestamp":1688393419931,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"nOgFGrGfBcVI"},"outputs":[],"source":["\"\"\"\n","torchtext 라이브러리의 가장 기본적인 구성요소\n","\n","토크나이저 및 어휘집을 사용한 일반적인 NLP 데이터 처리의 예시 보기\n","    1. 가공되지 않은 학습 데이터셋으로 어휘집 생성\n","        ㄴ 토큰의 목록 또는 반복자를 받는 내장 팩토리 함수 build_vocab_from_iterator 사용\n","        ㄴ 사용자는 어휘집에 추가할 특수 기호를 전달할 수도 있다\n","\"\"\"\n","\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","tokenizer = get_tokenizer('basic_english')\n","train_iter = AG_NEWS(split='train')\n","\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","# iterator를 이용하여 vocab 만들기\n","vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n","\n","# OOV token에 대해서 인덱스 부여\n","vocab.set_default_index(vocab[\"<unk>\"])"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1688393603959,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"DWv2wfvvFvXh","outputId":"a7006797-fbdf-4107-d748-ff8864e19ead"},"outputs":[{"data":{"text/plain":["[475, 21, 30, 5297]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# 어휘집 블록은 토큰 목록을 정수로 변환한다\n","vocab(['here', 'is', 'an', 'example'])"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1688393665633,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"rSbv7FgJGdjN"},"outputs":[],"source":["# 토크나이저, 어휘집을 갖춘 텍스트 처리 파이프라인 준비\n","# 텍스트/레이블 파이프라인은 데이터셋 반복자로부터 얻어온 raw한 문장 데이터를 처리하기 위해 사용\n","text_pipeline = lambda x: vocab(tokenizer(x))\n","label_pipeline = lambda x: int(x) - 1"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295,"status":"ok","timestamp":1688393730780,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"NQ2B4Ci_GsnJ","outputId":"8b956c39-fa17-442f-d03e-c1797e19cc5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[475, 21, 2, 30, 5297]\n","9\n"]}],"source":["\"\"\"\n","텍스트 파이프라인\n","    vocab에 정의된 룩업 테이블에 기반해 텍스트 문장을 정수 목록으로 변환\n","    레이블 파이프라인은 레이블을 정수로 변환\n","\"\"\"\n","\n","# 예)\n","print(text_pipeline('here is the an example'))\n","print(label_pipeline('10'))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BLpccpzYG8gD"},"source":["## 데이터 배치 및 반복자 생성"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1688394516884,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"0fOeuAsnHBkC"},"outputs":[],"source":["\"\"\"\n","getitem() 및 len() 프로토콜을 구현한 맵 형태의 데이터셋으로 동작\n","\n","모델로 보내기 전, collate_fn 함수는 DataLoader로부터 생성된 샘플 배치로 동작\n","collate_fn\n","    입력: DataLoader에 배치 크기가 있는 배치 데이터\n","    입력을 미리 선언된 데이터 처리 파이프라인에 따라 처리\n","\n","예)\n","1. 주어진 데이터 배치의 텍스트 항목들은 리스트에 담긴다\n","2. 리스트에 담긴 텍스트 항목들은 nn.Embedding의 입력을 위한 하나의 tensor로 합쳐진다\n","\n","* offset은 텍스트 tensor에서 개별 시퀀스 시작 인덱스를 표현하기 위한 구분자이다.\n","* 레이블은 개별 텍스트 항목의 레이블을 저장하는 tensor이다.\n","\"\"\"\n","\n","from torch.utils.data import DataLoader\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def collate_batch(batch):\n","    # offset: 텍스트 tensor에서 개별 시퀀스 시작 인덱스를 표현하기 위한 구분자\n","    label_list, text_list, offsets = [], [], [0]\n","\n","    for (_label, _text) in batch:\n","        label_list.append(label_pipeline(_label))\n","        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","        text_list.append(processed_text)\n","        offsets.append(processed_text.size(0))\n","\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","\n","    # cumsum: cumulative sum\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","\n","    # nn.Embedding의 입력을 위한 하나의 Tensor로 합치는 과정\n","    text_list = torch.cat(text_list)\n","    return label_list.to(device), text_list.to(device), offsets.to(device)\n","\n","train_iter = AG_NEWS(split='train')\n","dataloader = DataLoader(\n","    train_iter,\n","    batch_size=8,\n","    shuffle=False,\n","    collate_fn = collate_batch\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2lgyWX4IKSr5"},"source":["## 모델 정의\n","\n","모델은 nn.EmbeddingBag 레이어 및 분류를 위한 선형 레이어로 구성된다\n","\n","`nn.EmbeddingBag`은 기본으로 가방의 평균 값을 계산한다\n","\n","텍스트의 길이를 offset으로 저장하고 있으므로, 패딩이 필요하지 않다\n","\n","![](https://tutorials.pytorch.kr/_images/text_sentiment_ngrams_model.png)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":374,"status":"ok","timestamp":1688394968795,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"rtss9JI8KhXU"},"outputs":[],"source":["from torch import nn\n","\n","class TextClassificationModel(nn.Module):\n","    def __init__(\n","        self,\n","        vocab_size,\n","        embed_dim,\n","        num_class,\n","    ):\n","        super(TextClassificationModel, self).__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","        # text: size of the dictionary of embeddings\n","        # offsets: size of each embedding vector\n","        embedded = self.embedding(text, offsets)\n","        return self.fc(embedded)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ekWb4yDVLqu5"},"source":["## 인스턴스 생성\n","\n","4종류의 레이블, 클래스의 개수도 4개이다\n","\n","```\n","1 : World (세계)\n","2 : Sports (스포츠)\n","3 : Business (경제)\n","4 : Sci/Tec (과학/기술)\n","```"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3208,"status":"ok","timestamp":1688395099712,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"WMrI6koRLzlM"},"outputs":[],"source":["# 임베딩 차원이 64인 모델 만들기\n","# 어휘집 크기 = 어휘집 길이, 클래스 개수 = 레이블 개수\n","\n","train_iter = AG_NEWS(split='train')\n","num_class = len(set([label for (label, text) in train_iter]))\n","vocab_size = len(vocab)\n","emsize = 64\n","model = TextClassificationModel(\n","    vocab_size,\n","    emsize,\n","    num_class\n",").to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-WoDSPYgMJ2U"},"source":["## 모델을 학습하고 결과 평가하는 함수 정의"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":260,"status":"ok","timestamp":1688396204264,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"GsxGGzAcMMSM"},"outputs":[],"source":["import time\n","\n","def train(dataloader):\n","    model.train()\n","    total_acc, total_count = 0, 0\n","    log_interval = 500\n","\n","    start_time = time.time()\n","\n","    for idx, (label, text, offsets) in enumerate(dataloader):\n","        optimizer.zero_grad()\n","        predicted_label = model(text, offsets)\n","        loss = criterion(predicted_label, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","        optimizer.step()\n","\n","        total_acc += (predicted_label.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}|{:5d} batches '\n","                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader), total_acc/total_count))\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","def evaluate(dataloader):\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (label, text, offsets) in enumerate(dataloader):\n","            predicted_label = model(text, offsets)\n","            loss = criterion(predicted_label, label)\n","            total_acc += (predicted_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc / total_count"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EaoLkPSyNQkE"},"source":["## 데이터셋을 분할하고 모델 수행"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116280,"status":"ok","timestamp":1688396321604,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"lrWWFAooNVO8","outputId":"ccc6dbea-a689-4158-89c9-e610781cc640"},"outputs":[{"name":"stdout","output_type":"stream","text":["| epoch   1 |   500| 1782 batches | accuracy    0.678\n","| epoch   1 |  1000| 1782 batches | accuracy    0.856\n","| epoch   1 |  1500| 1782 batches | accuracy    0.876\n","------------------------------------------------------------\n","| end of epoch   1 | time: 10.41s |valid accuracy    0.885\n","------------------------------------------------------------\n","| epoch   2 |   500| 1782 batches | accuracy    0.898\n","| epoch   2 |  1000| 1782 batches | accuracy    0.901\n","| epoch   2 |  1500| 1782 batches | accuracy    0.900\n","------------------------------------------------------------\n","| end of epoch   2 | time: 10.39s |valid accuracy    0.900\n","------------------------------------------------------------\n","| epoch   3 |   500| 1782 batches | accuracy    0.916\n","| epoch   3 |  1000| 1782 batches | accuracy    0.914\n","| epoch   3 |  1500| 1782 batches | accuracy    0.913\n","------------------------------------------------------------\n","| end of epoch   3 | time: 11.35s |valid accuracy    0.906\n","------------------------------------------------------------\n","| epoch   4 |   500| 1782 batches | accuracy    0.924\n","| epoch   4 |  1000| 1782 batches | accuracy    0.923\n","| epoch   4 |  1500| 1782 batches | accuracy    0.922\n","------------------------------------------------------------\n","| end of epoch   4 | time: 12.62s |valid accuracy    0.908\n","------------------------------------------------------------\n","| epoch   5 |   500| 1782 batches | accuracy    0.929\n","| epoch   5 |  1000| 1782 batches | accuracy    0.931\n","| epoch   5 |  1500| 1782 batches | accuracy    0.930\n","------------------------------------------------------------\n","| end of epoch   5 | time: 12.13s |valid accuracy    0.905\n","------------------------------------------------------------\n","| epoch   6 |   500| 1782 batches | accuracy    0.941\n","| epoch   6 |  1000| 1782 batches | accuracy    0.944\n","| epoch   6 |  1500| 1782 batches | accuracy    0.941\n","------------------------------------------------------------\n","| end of epoch   6 | time: 13.13s |valid accuracy    0.911\n","------------------------------------------------------------\n","| epoch   7 |   500| 1782 batches | accuracy    0.946\n","| epoch   7 |  1000| 1782 batches | accuracy    0.943\n","| epoch   7 |  1500| 1782 batches | accuracy    0.943\n","------------------------------------------------------------\n","| end of epoch   7 | time: 12.33s |valid accuracy    0.910\n","------------------------------------------------------------\n","| epoch   8 |   500| 1782 batches | accuracy    0.947\n","| epoch   8 |  1000| 1782 batches | accuracy    0.944\n","| epoch   8 |  1500| 1782 batches | accuracy    0.945\n","------------------------------------------------------------\n","| end of epoch   8 | time: 11.01s |valid accuracy    0.911\n","------------------------------------------------------------\n","| epoch   9 |   500| 1782 batches | accuracy    0.945\n","| epoch   9 |  1000| 1782 batches | accuracy    0.946\n","| epoch   9 |  1500| 1782 batches | accuracy    0.945\n","------------------------------------------------------------\n","| end of epoch   9 | time: 12.33s |valid accuracy    0.910\n","------------------------------------------------------------\n","| epoch  10 |   500| 1782 batches | accuracy    0.944\n","| epoch  10 |  1000| 1782 batches | accuracy    0.945\n","| epoch  10 |  1500| 1782 batches | accuracy    0.946\n","------------------------------------------------------------\n","| end of epoch  10 | time:  9.09s |valid accuracy    0.910\n","------------------------------------------------------------\n"]}],"source":["\"\"\"\n","원본 AG_NEWS에는 val 데이터가 포함되어 있지 않다\n","    ㄴ 학습 95: 검증 5로 분리하기\n","    ㄴ torch.utils.data.dataset.random_split 함수 사용하여 분리\n","\n","CrossEntropyLoss: nn.LogSoftmax() + nn.NLLLoss()\n","\"\"\"\n","\n","from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","\n","EPOCHS = 10\n","LR = 5\n","BATCH_SIZE = 64\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","\n","total_accu = None\n","train_iter, test_iter = AG_NEWS()\n","\n","# convert iterable-style dataset to map-style dataset\n","train_dataset = to_map_style_dataset(train_iter)\n","test_dataset = to_map_style_dataset(test_iter)\n","\n","num_train = int(len(train_dataset) * 0.95)\n","split_train_, split_valid_ = random_split(\n","    # [num_train, num_val] 로 개수 설정\n","    train_dataset, [num_train, len(train_dataset) - num_train]\n",")\n","\n","train_dataloader = DataLoader(\n","    split_train_,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    collate_fn=collate_batch\n",")\n","\n","valid_dataloader = DataLoader(\n","    split_valid_,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    collate_fn=collate_batch\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    collate_fn=collate_batch\n",")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","\n","    if total_accu is not None and total_accu > accu_val:\n","        scheduler.step()\n","    else:\n","        total_accu = accu_val\n","    print('-' * 60)\n","    print('| end of epoch {:3d} | time: {:5.2f}s |'\n","          'valid accuracy {:8.3f}'.format(epoch,\n","                                          time.time() - epoch_start_time,\n","                                          accu_val)\n","    )\n","    print('-' * 60)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XoRN1ic8QIuO"},"source":["## 임의의 뉴스로 평가하기"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1688396394141,"user":{"displayName":"양의동","userId":"09303157687264482652"},"user_tz":-540},"id":"5_lwEZOMQfNN","outputId":"f2fe5d4f-34d6-445e-92f0-28eadb68e175"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is a Sports news\n"]}],"source":["# 다른 뉴스로 테스트하기(골프 뉴스)\n","ag_news_label = {\n","    1: \"World\",\n","    2: \"Sports\",\n","    3: \"Business\",\n","    4: \"Sci/Tec\"\n","}\n","\n","def predict(text, text_pipeline):\n","    with torch.no_grad():\n","        text = torch.tensor(text_pipeline(text))\n","        output = model(text, torch.tensor([0]))\n","        return output.argmax(1).item() + 1\n","\n","ex_text_str = \"MEMPHIS, Tenn. - Four days ago, Jon Rahm was \\\n","    enduring the season’s worst weather conditions on Sunday at The \\\n","    Open on his way to a closing 75 at Royal Portrush, which \\\n","    considering the wind and the rain was a respectable showing. \\\n","    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n","    was another story. With temperatures in the mid-80s and hardly any \\\n","    wind, the Spaniard was 13 strokes better in a flawless round. \\\n","    Thanks to his best putting performance on the PGA Tour, Rahm \\\n","    finished with an 8-under 62 for a three-stroke lead, which \\\n","    was even more impressive considering he’d never played the \\\n","    front nine at TPC Southwind.\"\n","\n","model = model.to(\"cpu\")\n","\n","print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPcwNH0MkV9tA+hneLN7fpY","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
