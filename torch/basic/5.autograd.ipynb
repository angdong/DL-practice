{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNG/6FCXZW28l9NOJEqzrap"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[Torch.Autograd를 사용한 자동 미분](https://tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial.html)"],"metadata":{"id":"29rf7tdP2RZp"}},{"cell_type":"code","source":["\"\"\"\n","역전파 알고리즘\n","    매개변수는 주어진 매개변수에 대한 손실 함수의 변화도(gradient)에 따라 조정됨\n","    변화도 계산을 위한 자동 미분 엔진: torch.autograd\n","        모든 계산 그래프에 대한 변화도의 자동 계산 지원\n","\n","예) 입력 x, 매개변수 w와 b, 간단한 단일 계층 신경망\n","\"\"\"\n","\n","import torch\n","\n","x = torch.ones(5)\n","y = torch.zeros(3)\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w) + b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"],"metadata":{"id":"hC3s0gBu7j3g","executionInfo":{"status":"ok","timestamp":1687786927576,"user_tz":-540,"elapsed":4407,"user":{"displayName":"양의동","userId":"09303157687264482652"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Tensor, Function과 연산그래프\n","\n","![](https://tutorials.pytorch.kr/_images/comp-graph.png)\n","\n","w, b는 최적화를 해야 하는 매개변수\\\n","변수들에 대한 손실 함수의 변화도 계산을 위해 `requires_grad` 속성 설정"],"metadata":{"id":"zLh5a-708KCV"}},{"cell_type":"code","source":["print(f\"Gradient function for z = {z.grad_fn}\")\n","print(f\"Gradient function for loss = {loss.grad_fn}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-_OqWjs8XUQ","executionInfo":{"status":"ok","timestamp":1687787028013,"user_tz":-540,"elapsed":6,"user":{"displayName":"양의동","userId":"09303157687264482652"}},"outputId":"305d1aa9-6bfe-446f-b2e2-ad29d52c1280"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient function for z = <AddBackward0 object at 0x7fed72612e00>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fed726114b0>\n"]}]},{"cell_type":"markdown","source":["## 변화도 계산"],"metadata":{"id":"wiNa4YDu8jpy"}},{"cell_type":"code","source":["\"\"\"\n","신경망에서 매개변수의 가중치 최적화를 위해 손실함수의 도함수를 계산\n","도함수 계산을 위해 loss.backward() 호출한 뒤, w.grad, b.grad에서 값을 가져옴\n","\"\"\"\n","\n","loss.backward()\n","print(w.grad)\n","print(b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YGSCfrrQ8pFN","executionInfo":{"status":"ok","timestamp":1687787121809,"user_tz":-540,"elapsed":5,"user":{"displayName":"양의동","userId":"09303157687264482652"}},"outputId":"579c1704-ec0c-4857-8d7a-07bf11cb0c26"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2793, 0.3094, 0.0068],\n","        [0.2793, 0.3094, 0.0068],\n","        [0.2793, 0.3094, 0.0068],\n","        [0.2793, 0.3094, 0.0068],\n","        [0.2793, 0.3094, 0.0068]])\n","tensor([0.2793, 0.3094, 0.0068])\n"]}]},{"cell_type":"markdown","source":["## 변화도 추적 멈추기"],"metadata":{"id":"Y55heRoJ86gw"}},{"cell_type":"code","source":["\"\"\"\n","모델 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우의 경우에는 추적 필요 없다\n","연산 코드를 torch.no_grad() 블록으로 둘러싸서 연산 추적 멈출 수 있다\n","\"\"\"\n","\n","z = torch.matmul(x, w) + b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w) + b\n","print(z.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BG8D0UiA9vpw","executionInfo":{"status":"ok","timestamp":1687787394406,"user_tz":-540,"elapsed":5,"user":{"displayName":"양의동","userId":"09303157687264482652"}},"outputId":"4096c9fe-9e58-4b1f-b713-37e588bb1280"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"code","source":["# 다른 방법: detach() 메소드 사용\n","z = torch.matmul(x, w) + b\n","z_det = z.detach()\n","print(z_det.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fVtyOaL999DB","executionInfo":{"status":"ok","timestamp":1687787440085,"user_tz":-540,"elapsed":5,"user":{"displayName":"양의동","userId":"09303157687264482652"}},"outputId":"1ddd8e5e-8441-4d69-9a3a-2a668c8df7eb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","source":["## 연산 그래프에 대한 추가 정보\n","\n","`autograd`는 텐서의 실행된 모든 연산들의 기록을 DAG에 저장\\\n","DAG의 잎은 입력 텐서이고, 뿌리는 결과 텐서\\\n","뿌리에서부터 잎까지 chain rule에 따라 변화도를 자동으로 계산\n","\n","* 순전파 단계에서 autograd의 작업\n","\n","1. 요청된 연산 수행하여 결과 텐서 계산\n","2. DAG에 연산의 변화도 기능을 유지\n","\n","* 역전파 단계는 DAG의 뿌리에서 `.backward()` 호출시 시작\n","\n","1. 각 `.grad_fn`으로부터 변화도 계산\n","2. 각 텐서의 `.grad` 속성에 계산 결과 쌓기\n","3. 연쇄 법칙 사용해서 모든 잎 텐서들까지 전파"],"metadata":{"id":"rdFslcyf-JKw"}},{"cell_type":"markdown","source":["## 선택적으로 읽기: 텐서 변화도와 야코비안 곱"],"metadata":{"id":"ahjmea2i-uyW"}},{"cell_type":"code","source":["\"\"\"\n","대부분의 경우 스칼라 손실 함수를 이용해서 일부 매개변수와 관련한 변화도 계산\n","\n","출력 함수가 임의의 텐서인 경우에는?\n","    PyTorch는 실제 변화도가 아닌 야코비안 곱을 계산\n","    야코비안 행렬 자체를 계산하는 대신 주어진 입력벡터에 대한 야코비안 곱을 구함\n","\"\"\"\n","\n","inp = torch.eye(4, 5, requires_grad=True)\n","out = (inp+1).pow(2).t()\n","out.backward(torch.ones_like(out), retain_graph=True)\n","print(f\"First call\\n{inp.grad}\")\n","\n","# PyTorch가 변화도를 누적시키기 때문에 값이 달라진다\n","out.backward(torch.ones_like(out), retain_graph=True)\n","print(f\"\\nSecond call\\n{inp.grad}\")\n","\n","# grad 속성을 0으로 만들어서 변화도 누적 방지\n","# 실제 학습 과정에서는 옵티마이저가 해당 과정 도와줌\n","inp.grad.zero_()\n","out.backward(torch.ones_like(out), retain_graph=True)\n","print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nmi5reZx-5Y0","executionInfo":{"status":"ok","timestamp":1687787922909,"user_tz":-540,"elapsed":423,"user":{"displayName":"양의동","userId":"09303157687264482652"}},"outputId":"607d4f15-1517-48b2-d88c-446723812dbf"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["First call\n","tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.]])\n","\n","Second call\n","tensor([[8., 4., 4., 4., 4.],\n","        [4., 8., 4., 4., 4.],\n","        [4., 4., 8., 4., 4.],\n","        [4., 4., 4., 8., 4.]])\n","\n","Call after zeroing gradients\n","tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.]])\n"]}]}]}